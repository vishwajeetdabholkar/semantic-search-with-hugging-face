{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e19358e-22e8-406c-ae17-d916db889313",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(210, 255, 153, 0.25); padding: 5px;\">\n",
        "    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n",
        "        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/chart-network.png\" />\n",
        "    </div>\n",
        "    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n",
        "        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n",
        "        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">Semantic Search with Hugging Face Models and Datasets</h1>\n",
        "    </div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bebf253-7913-4d7a-8ebc-f10463803baa",
      "metadata": {},
      "source": "In this notebook, we will demonstrate an example of conducting semantic search on SingleStoreDB with SQL! Unlike traditional keyword-based search methods, semantic search algorithms take into account the relationships between words and their meanings, enabling them to deliver more accurate and relevant results \u2013 even when search terms are vague or ambiguous. \n\nSingleStoreDB\u2019s built-in parallelization and Intel SIMD-based vector processing takes care of the heavy lifting involved in processing vector data. This allows your to run your ML algorithms right in your database extremely efficiently with just 1 line of SQL!\n\nIn this example, we use Hugging Face to create embeddings for our dataset and run semantic_search using dot_product vector matching function!"
    },
    {
      "cell_type": "markdown",
      "id": "358d1eb0-a0dd-423d-86ea-0d131abe4169",
      "metadata": {},
      "source": "## 1. Create a workspace in your workspace group\n\nS-00 is sufficient.\n\n## 2. Create a database named `semantic_search`"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af5e02fb-e15b-4c85-ac69-a40dd974cd88",
      "metadata": {},
      "outputs": [],
      "source": "%%sql\nDROP DATABASE IF EXISTS semantic_search;\n\nCREATE DATABASE semantic_search;"
    },
    {
      "cell_type": "markdown",
      "id": "284f2bdc-a428-4a55-9f1f-fce623914b34",
      "metadata": {},
      "source": "<div class=\"alert alert-block alert-warning\">\n    <b class=\"fa fa-solid fa-exclamation-circle\"></b>\n    <div>\n        <p><b>Action Required</b></p>\n        <p>Make sure to select the <tt>semantic_search</tt> database from the drop-down menu at the top of this notebook.\n        It updates the <tt>connection_url</tt> which is used by the <tt>%%sql</tt> magic command and SQLAlchemy to make connections to the selected database.</p>\n    </div>\n</div>"
    },
    {
      "cell_type": "markdown",
      "id": "8124ab1c-7f17-47bc-9f8a-c7bd5a33a426",
      "metadata": {},
      "source": "## 3. Install and import required libraries\n\nWe will use an embedding model on Hugging Face with Sentence Transfomers library. We will be analysing the sentiment of reviewers of selected movies. This dataset is available on Hugging Face and to use it, we will need the datasets library. \n\nThe install process may take a couple minutes."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6146b2-a044-4dd8-b020-e3d8c1f91aba",
      "metadata": {},
      "outputs": [],
      "source": "!pip3 install --upgrade sentence-transformers torch tensorflow datasets pandarallel --quiet\n\nimport json\n\nimport ibis\nimport numpy as np\nimport pandas as pd\nimport sqlalchemy as sa\nimport singlestoredb as s2\nimport torch\n\nfrom datasets import load_dataset\nfrom pandarallel import pandarallel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\n\npandarallel.initialize(nb_workers=2, progress_bar=True)"
    },
    {
      "cell_type": "markdown",
      "id": "f80d23bc-7e98-4ac8-b2a0-7a737e4010e5",
      "metadata": {},
      "source": "## 4. Load Sentence Transformer library and create a function called `get_embedding()`\n\nTo vectorize and embed the reviews that watchers of the movies left, we will be using the `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` model. We will create a function called `get_embedding()` that will call this model and return the vectorized version of the sentence."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a463c0fd-c747-4605-a728-c22a2fa17cfb",
      "metadata": {},
      "outputs": [],
      "source": "# Load Sentence Transformers model\nmodel_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)"
    },
    {
      "cell_type": "markdown",
      "id": "bac82174-a2a3-40aa-b843-235a7547cace",
      "metadata": {},
      "source": "Add a function to compute the embedding. The result will be a numpy array of 32-bit floats."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e31300-1e6a-425c-bcf7-3708ce9e40d0",
      "metadata": {},
      "outputs": [],
      "source": "def get_embedding(sentence: str) -> np.ndarray[np.float32]:\n    \"\"\"Retrieve embedding for given sentence.\"\"\"\n    inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n    with torch.no_grad():\n        embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().tolist()\n    return np.array(embedding, dtype='<f4')"
    },
    {
      "cell_type": "markdown",
      "id": "17fb3aad-e3a8-4a2a-985c-64f0c94431b8",
      "metadata": {},
      "source": "## 5. Load the dataset on movie reviews from Hugging Face into a `DataFrame`\n\nWe will be doing some operations and only sampling 100 random reviews from the \"test\" dataset of `imdb-movie-reviews`."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3af3810-0ce5-432b-a879-4eaa16524d38",
      "metadata": {},
      "outputs": [],
      "source": "# Load the dataset into a pandas DataFrame\ndataset = load_dataset(\"ajaykarthick/imdb-movie-reviews\")\ndataframe = dataset[\"train\"].to_pandas()\n\nsample_size = 100  # Adjust the desired sample size\nrandom_sample = dataframe.sample(n=sample_size)"
    },
    {
      "cell_type": "markdown",
      "id": "07a15a81-1312-4984-b7d9-8539a64c719f",
      "metadata": {},
      "source": "## 6. Generate embeddings of the reviews left by customers and add them to your `DataFrame`\n\nWe want to embed the entries in the `review` column and add the embeddings to the data. We will do this with pandas and our `get_embeddings` function. Embeddings are stored as a numpy array."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2749ea9d-de63-4058-b682-faa9ec002d60",
      "metadata": {},
      "outputs": [],
      "source": "random_sample['review_embeddings'] = random_sample['review'].parallel_apply(get_embedding)"
    },
    {
      "cell_type": "markdown",
      "id": "8188ccb2-d5cf-48b5-8c9f-8b3858c18ae7",
      "metadata": {},
      "source": "## 7. Insert data into SingleStoreDB\n\nYou can seamlessly bring this data to your SingleStoreDB table directly from your from `DataFrame`. SingleStore \u2665\ufe0f Python.\n\nWe will bring this data into a table called `reviews`. Notice how you don't have to write any SQL for this\u00a0\u2013\u00a0we will infer the schema from your `DataFrame` and underneath the hood configure how to bring this `DataFrame` into our database. Since numpy arrays don't map directly to a database type, we give pandas a type hint to create a blob column for the `review_embeddings` column. "
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bd03799-b221-4dcd-9ce7-2db2b7ec1a8e",
      "metadata": {},
      "outputs": [],
      "source": "random_sample.to_sql('reviews',\n                     s2.create_engine().connect(), \n                     if_exists='replace', \n                     index=False,\n                     dtype=dict(review_embeddings=sa.LargeBinary))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f900f5-eb39-413e-b414-a29cfafa60ef",
      "metadata": {},
      "outputs": [],
      "source": "# Create a database connection and display the `CREATE TABLE` statement\nconn = s2.connect()\n\nconn.show.create_table('reviews')"
    },
    {
      "cell_type": "markdown",
      "id": "e34e62fb-7690-4a31-a874-ff7856d16cc7",
      "metadata": {},
      "source": "## 8. Run the semantic search algorithm with just one line of SQL\n\nWe will utilize SingleStoreDB's distributed architecture to efficiently compute the dot product of the input string (stored in searchstring) with each entry in the database and return the top 5  reviews with the highest dot product score. Each vector is normalized to length 1, hence the dot product function essentially computes the cosine similarity between two vectors \u2013 an appropriate nearness metric. SingleStoreDB makes this extremely fast because it compiles queries to machine code and runs dot_product using SIMD instructions."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08bd6b1c-9731-4062-9b9a-a5e1a1d8efa3",
      "metadata": {},
      "outputs": [],
      "source": "searchstring = input('Please enter a search string:')\n\nsearch_embedding = get_embedding(searchstring).tobytes().hex()\n\nresults = %sql SELECT review, DOT_PRODUCT(review_embeddings, X'{{search_embedding}}') AS Score \\\n               FROM reviews ORDER BY Score DESC LIMIT 5;\n\nprint()\nfor i, res in enumerate(results):\n    print(f'{i + 1}: {res[0]} Score: {res[1]:.2f}\\n')"
    },
    {
      "cell_type": "markdown",
      "id": "0383939d-7fd3-434d-a27b-952eeed40e5f",
      "metadata": {},
      "source": "## 9. Clean up"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e91592f-4856-4cab-b15e-23585f551ab3",
      "metadata": {},
      "outputs": [],
      "source": "%%sql\nDROP DATABASE semantic_search;"
    },
    {
      "cell_type": "markdown",
      "id": "a6829f66-b37e-493d-9631-6da519140485",
      "metadata": {},
      "source": [
        "<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n",
        "<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
